---
title: "Inference of subclonal architecture of tumors across multiple timepoints for the PBTA Cohort"
author: 'Antonia Chroni <chronia@chop.edu> and Jo Lynne Rokita <rokita@chop.edu> for D3B'
date: "2023"
output:
  html_notebook:
    toc: TRUE
    toc_float: TRUE
---


#### Tumor evolution project 

# Background
We are investigating the subclonal architecture of tumors across multiple timepoints in the PBTA cohort.
We will infer fishplots for each patient sample with multiple time points.

### Data used 
In this notebook, we are looking into the v12 histologies file (last updates on May 1st, 2023) and subset to the PBTA cohort.
We use the output files from the "analyses/sample-distribution-analysis/results".

### Method used 
For visualizing the tumor sublcones, we will use the fishplot R package as described in here: https://github.com/chrisamiller/fishplot


### Usage

This notebook is intended to be run via the command line from the top directory
of the repository as follows:

```
Rscript -e "rmarkdown::render('01-process-data.Rmd', clean = TRUE)"
```

# Set up

```{r sload-library}
suppressPackageStartupMessages({
  library(tidyverse)
  library(data.table)
  library(R.utils)
  library(clonevol)
  library(devtools)
  library(purrr)
  library(flextable)
  library(plyr)
  library(future)
})

# run processes asynchronously when possible using future package
plan("multisession", workers = 4)
# increase the limit of the data to be shuttled between the processes from default 500MB to 50GB
options(future.globals.maxSize = 30 * 1024 ^ 3)
```

## Directories and File Inputs/Outputs

```{r set-dir-and-file-names}
# Detect the ".git" folder -- this will be in the project root directory.
# Use this as the root directory to ensure proper sourcing of functions no
# matter where this is called from
root_dir <- rprojroot::find_root(rprojroot::has_dir(".git"))
setwd(root_dir)

analysis_dir <- file.path(root_dir, "analyses", "tumor-clone-inference")

# File path to input directory
input_dir <-
  file.path(analysis_dir, "input")

# Inputs
input_snv <- file.path(root_dir, "data/snv-consensus-plus-hotspots.maf.tsv.gz")

# The following files were generated from the add-sample-distribution analysis
# to source files from there - TO FIX THIS LATER
input_cohort_pbta <- file.path(input_dir, "pbta.tsv")
input_genomic_df_filter <- file.path(input_dir, "genomic_df_filter.tsv")

# File path to results directory
results_dir <-
  file.path(analysis_dir, "results")
if (!dir.exists(results_dir)) {
  dir.create(results_dir)
}


# File path to plots directory
plots_dir <-
   file.path(analysis_dir, "plots")
 if (!dir.exists(plots_dir)) {
   dir.create(plots_dir)
 }
```


## Load data
```{r load-inputs-please-wait}
# Inputs
## Read MAF file
snv <- data.table::fread(input_snv, data.table = F)

pbta <- readr::read_tsv(input_cohort_pbta, guess_max = 100000, show_col_types = FALSE)

## Read in genomic_df_filter file
genomic_df_filter <- readr::read_tsv(input_genomic_df_filter, guess_max = 100000, show_col_types = FALSE)
```


# Process Data: snv 

We need to process each input file and generate input files for the fishplot analysis.
We will first change the column name of the biospecimen column to match the one from the histologies files.

```{r process snv}
colnames(snv)[which(names(snv) == "Tumor_Sample_Barcode")] <- "Kids_First_Biospecimen_ID"
```  


```{r subset snv}
# we will subset to variables we will interested to focus on for downstream analysis
snv_subset <- subset(snv,
                     select = c(Hugo_Symbol, Kids_First_Biospecimen_ID, Chromosome, Start_Position, End_Position, Reference_Allele, Tumor_Seq_Allele1, Tumor_Seq_Allele2, Variant_Type, Gene, t_ref_count, t_alt_count)) 
```

# Process Data for CloneFinder

We need to generate the input files for CloneFinder according to the method's template.

## Step 1

First though, we need to ensure that we have enough number of tumor samples per time point and patient case.
This is because CloneFinder requires at least 2 tumor samples per anatomical site.
Thus, we will first identify the number of tumor samples per patient and we will keep only those with more than 2 tumor samples.


```{r Process Data for CloneFinder-Step1}
# Merge genomic_df_filter with pbta_subset to get sample_id information
# Create match_id2 to count for multiple samples per patient, if any

genomic_df_filter_merge <- genomic_df_filter %>% 
  left_join(pbta, by=c('Kids_First_Participant_ID'))

genomic_df_subset <- genomic_df_filter_merge %>%
  subset(select = c(Kids_First_Participant_ID, Kids_First_Biospecimen_ID, sample_id, composition, match_id, tumor_descriptor, descriptors, cancer_group.x, experimental_strategy.y)) %>%
  filter(!(experimental_strategy.y == "RNA-Seq")) %>%
  mutate(match_id2 = paste(Kids_First_Participant_ID, sample_id, composition, sep = "_")) %>%
  mutate(tumor_descriptor = case_when(grepl("Primary Tumor", tumor_descriptor) ~ "Diagnosis",
                                 grepl("Initial CNS Tumor", tumor_descriptor) ~ "Diagnosis",
                                 grepl('Progressive Disease Post-Mortem', tumor_descriptor) ~ 'Deceased', 
                                 TRUE ~ tumor_descriptor)) %>%
  write_tsv(file.path(results_dir, "genomic_df_subset.tsv"))

genomic_df_subset <- genomic_df_subset %>%
  select(Kids_First_Participant_ID, match_id2, Kids_First_Biospecimen_ID, descriptors, tumor_descriptor, cancer_group.x, experimental_strategy.y)  %>%
  arrange(Kids_First_Participant_ID) 


genomic_df_subset %>%
  group_by(Kids_First_Participant_ID, match_id2, descriptors) %>%
  tally() %>%
  regulartable() %>%
  fontsize(size = 12, part = "all")


# the following shows number of samples per descent order
genomic_df_subset %>%
  group_by(Kids_First_Participant_ID, match_id2, descriptors) %>%
  tally() %>%
  arrange(desc(n)) %>% 
  regulartable() %>%
  fontsize(size = 12, part = "all")

# let's count tumor samples/patient
count <- genomic_df_subset %>% 
         group_by(Kids_First_Participant_ID, match_id2, Kids_First_Biospecimen_ID, descriptors, tumor_descriptor, cancer_group.x) %>%
         dplyr::count(Kids_First_Participant_ID, tumor_descriptor) 

count %>%
  group_by(Kids_First_Participant_ID, tumor_descriptor, descriptors) %>%
  tally() %>%
  regulartable() %>%
  fontsize(size = 12, part = "all")


# keep only the rows with more than 2 occurrences in count$n
# to ensure we have at least 2 tumor samples per patient case and tumor_descriptor
#keep <- count[count$n > 1, ] %>%
#  write_tsv(file.path(results_dir, "genomic_df_subset_keep.tsv"))

# TODO TO FIX THIS PART
# TODO ALSO THIS SAMPLE PT_MNSEJCDM HAAS 2 ENTRIES 
#keep <- count %>%
#  filter(Kids_First_Participant_ID %in% c("PT_GTHZF21E", "PT_23NZGSRJ", "PT_3KM9W8S8", "PT_HFQNKP5X", "PT_HJMP6PH2",
#                                          "PT_KZ56XHJT", "PT_Z4BF2NSB", "PT_19GCSK2S", "PT_KTRJ8TFY", "PT_6N825561",
#                                          "PT_75HRTX4S", "PT_KTRJ8TFY", "PT_KZ56XHJT", "PT_MNSEJCDM", "PT_NK8A49X5")) %>%
#  write_tsv(file.path(results_dir, "genomic_df_subset_keep.tsv"))


keep <- count %>%
  filter(Kids_First_Participant_ID %in% c("PT_KZ56XHJT", "PT_MNSEJCDM", "PT_KTRJ8TFY", "PT_NK8A49X5", "PT_S4YNE17X")) %>% # "PT_HFQNKP5X" for Deceased (6), Progressive (2), Recurrence (1)
  write_tsv(file.path(results_dir, "genomic_df_subset_keep.tsv"))
```

This results to 5 patients. BUT I had issues for automatically calculating the number of patients and so I did this manually.
Maybe there is a way to do computationally?

Anyway, in the next step, we will use this to filter and prepare input files for CloneFinder.

## Step 2
Input files for CloneFinder should contain the read counts for reference and alteration per each chromosomal position.
We will split these per patient sample based on the filtering from step 1.

```{r Process Data for CloneFinder-Step2}
# we will replace names in the column to follow input guidelines for CloneFinder

read_counts_all_samples <- subset(snv_subset, 
                             select = c(Kids_First_Biospecimen_ID, Chromosome, Start_Position, Reference_Allele, Tumor_Seq_Allele2, t_ref_count, t_alt_count))

read_counts_all_samples <- setnames(read_counts_all_samples, old = c('Chromosome','Start_Position','Reference_Allele', "Tumor_Seq_Allele2", "t_ref_count", "t_alt_count"), 
         new = c('Chr','Position','Wild', "Mut", "XX:ref", "XX:alt")) 

read_counts_all_samples_keep <- read_counts_all_samples %>% 
  left_join(keep, by=c('Kids_First_Biospecimen_ID')) %>%
  filter(!is.na(Kids_First_Participant_ID))
  
  
 # write_tsv(file.path(results_dir, "read_counts_all_samples_keep.tsv")) # this is a large file, if not needed in later stages, maybe there is no need to save
```

We will split and save read count data by the "Kids_First_Participant_ID" column

```{r echo=TRUE}
# Create dir to save read count data to be used as input for CloneFinder
temp_path_csv <-
  file.path(results_dir, "01_Read_count_Per_Patient")
if (!dir.exists(temp_path_csv)) {
  dir.create(temp_path_csv)
}

# we will change the dir here otherwise files are saved outside of the dsired folder
setwd(temp_path_csv)

# we will split Read_count data by the "Kids_First_Participant_ID.x" column
read_counts_all_samples_keep %>% 
  group_split(Kids_First_Participant_ID) %>% 
  walk(~write_csv(.x, paste0("01_", .x$Kids_First_Participant_ID[1], ".csv")))

# let's change back to the root_dir for the rest of the module
setwd(root_dir)
```

## Step 3 

Next we need to create different columns assigning the read counts for ref and alt per tumor sample ("Kids_First_Biospecimen_ID").
We will do this in three steps: (1) create wide df for ref variant, (2) create wide df for alt variant, and (3) merge wide df.

```{r Process Data for CloneFinder-Step3}
#create out_dir
    temp_path <-
      file.path(results_dir, "02_CloneFinder_input_files")
    if (!dir.exists(temp_path)) {
      dir.create(temp_path)
    } 

# create a list and read csv files
all_files <-  list.files(path = temp_path_csv, # Identify all CSV files
                       pattern = "*.csv", full.names = TRUE)
data_all <- lapply(all_files, read.csv)

##Create list of data frame names without the "CloneFinder_input_read_count_csv." part 
# get only the file names, not the full path.
names(data_all) <- gsub("01_", "",
                       list.files(temp_path_csv,full.names = FALSE),
                       fixed = TRUE)

# we will change the dir here otherwise files are saved outside of the desired folder
setwd(temp_path)

# create a loop for the elements in the 'all_files' list
# and create input files

for(i in 1:length(data_all)) {  
# i = "PT_KTRJ8TFY.csv"

# we need to have separate columns with the tumor sample containing read counts per variant
  
#####################
# we will first create a data frame for the read counts for the reference and alteration variant
# create unique identifier based on chromosome and chromosomal position for each variant
# let's also keep the information with tumor sample and time point together. This is useful for visualization purposes later.
# XX:ref  
  
data_all[[i]]$df <- data_all[[i]] %>%
       mutate(match_id_unique = paste(Chr, Position, sep = "_")) %>%
       mutate(Kids_First_Biospecimen_ID_descriptor = paste(Kids_First_Biospecimen_ID, tumor_descriptor, sep = "_"))

  
data_all[[i]]$df_ref <- data_all[[i]]$df %>%
       mutate(Kids_First_Biospecimen_ID_descriptor = paste(Kids_First_Biospecimen_ID_descriptor, sep = ":", "ref")) %>% 
       subset(select = c(match_id_unique, Kids_First_Biospecimen_ID_descriptor, XX.ref)) 

# then we need to transform the data from long to wide
data_all[[i]]$df_ref_wide = data_all[[i]]$df_ref %>%
       pivot_wider(names_from = "Kids_First_Biospecimen_ID_descriptor", values_from = "XX.ref", values_fn = list)


#####################
# then we need to transform the data from long to wide
# XX:alt
data_all[[i]]$df_alt <- data_all[[i]]$df %>%
       mutate(Kids_First_Biospecimen_ID_descriptor = paste(Kids_First_Biospecimen_ID_descriptor, sep = ":", "alt")) %>% 
       subset(select = c(match_id_unique, Kids_First_Biospecimen_ID_descriptor, XX.alt)) 

# then we need to transform the data from long to wide
data_all[[i]]$df_alt_wide = data_all[[i]]$df_alt %>%
       pivot_wider(names_from = Kids_First_Biospecimen_ID_descriptor, values_from = XX.alt, values_fn = list)

#####################
# merge data_all[[i]]$df_ref with data_all[[i]]$df_alt
# add columns: 'Chr','Position','Wild', "Mut" that are missing so it matches the CloneFinder input

data_all[[i]]$merge_read_counts <- list(data_all[[i]]$df_ref_wide, left_join(data_all[[i]]$df_alt_wide)) %>% 
                                          reduce(left_join, by=c('match_id_unique'))  %>% 
                                          mutate(Chr =gsub("_..*", "", match_id_unique)) %>% 
                                          mutate(Position =gsub("_", "", match_id_unique)) 

# we will replace any NAs with zero
data_all[[i]]$merge_read_counts[is.na(data_all[[i]]$merge_read_counts)] <- 0  
  
# save edited files
write.table(data_all[[i]]$merge_read_counts, paste0(temp_path, "_edited.txt"))
} 

# let's change back to the root_dir for the rest of the module
setwd(root_dir)
```

## Step 4 

```{r Process Data for CloneFinder-Step4}
# let's create a list of the samples and their directory path
# we will use this to run CloneFinder for all samples together

distinct_samples <- read_counts_all_samples_keep %>% distinct(Kids_First_Participant_ID)                   #removes the duplicate values
samples_list_path <-  subset(distinct_samples, 
                             select = c(Kids_First_Participant_ID)) %>%
           mutate(path = temp_path,
                 additional = 1) %>%
  write_csv(file.path(results_dir, "distinct_samples_list.csv"))
```


```{r echo=TRUE}
# Create directory to save CloneFinder results
CloneFinder_results <-
  file.path(results_dir, "CloneFinder_results")
if (!dir.exists(CloneFinder_results)) {
  dir.create(CloneFinder_results)
}
```

```{r}
sessionInfo()
```