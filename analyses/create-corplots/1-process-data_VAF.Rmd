---
title: "Estimation of VAFs of tumors across multiple timepoints for the PBTA Cohort"
author: 'Antonia Chroni <chronia@chop.edu> and Jo Lynne Rokita <rokita@chop.edu> for D3B'
date: "2023"
output:
  html_notebook:
    toc: TRUE
    toc_float: TRUE
---


#### Tumor evolution project 

# Background
We are investigating the change of VAFs across multiple timepoints in the PBTA cohort.

### Data used 
In this notebook, we are looking into the v12 histologies file (last updates on May 1st, 2023) and subset to the PBTA cohort.
We use the output files from the "analyses/sample-distribution-analysis/results".

### Method used 


### Usage

This notebook is intended to be run via the command line from the top directory
of the repository as follows:

```
Rscript -e "rmarkdown::render('analyses/create-corplots/01-process-data_VAF.Rmd', clean = TRUE)"
```

# Set up

```{r load-library}
suppressPackageStartupMessages({
  library(tidyverse)
  library(data.table)
  library(flextable)
  library(plyr)
})
```

## Directories and File Inputs/Outputs

```{r set-dir-and-file-names}
# Detect the ".git" folder -- this will be in the project root directory.
# Use this as the root directory to ensure proper sourcing of functions no
# matter where this is called from
root_dir <- rprojroot::find_root(rprojroot::has_dir(".git"))
# root_dir <- "/Users/chronia/CHOP/GitHub/pbta-tumor-evolution"
setwd(root_dir)

analysis_dir <- file.path(root_dir, "analyses", "create-pptc-pdx-corplots")

# File path to input directory
input_dir <-
  file.path(analysis_dir, "input")

# Inputs
input_snv <- file.path(root_dir, "data/snv-consensus-plus-hotspots.maf.tsv.gz")

# The following files were generated from the add-sample-distribution analysis
input_cohort_pbta <- file.path(input_dir, "pbta.tsv")
input_genomic_df_filter <- file.path(input_dir, "genomic_df_filter.tsv")

# File path to results directory
results_dir <-
  file.path(analysis_dir, "results")
if (!dir.exists(results_dir)) {
  dir.create(results_dir)
}

```

## Load data
```{r load-inputs-please-wait}
# Inputs
## Read MAF file
snv <- data.table::fread(input_snv, data.table = F)

pbta <- readr::read_tsv(input_cohort_pbta, guess_max = 100000, show_col_types = FALSE)

## Read in genomic_df_filter file
genomic_df_filter <- readr::read_tsv(input_genomic_df_filter, guess_max = 100000, show_col_types = FALSE)
```


# Process Data: snv 

We need to process each input file and generate input files for the fishplot analysis.
We will first change the column name of the biospecimen column to match the one from the histologies files.

```{r process snv}
colnames(snv)[which(names(snv) == "Tumor_Sample_Barcode")] <- "Kids_First_Biospecimen_ID"
```  

```{r calculate VAF, echo=TRUE}
# using formula for VAF calculation
snv_vaf <- snv %>% mutate(VAF = t_alt_count / (t_ref_count + t_alt_count))
```

```{r echo=TRUE}
# we will subset to variables we will interested to focus on for downstream analysis
snv_vaf_subset <- subset(snv_vaf, 
                             select = c(Kids_First_Biospecimen_ID, Hugo_Symbol,Chromosome, Start_Position, Variant_Classification,VAF))
```

* TODO *
The following code is part of the module "tumor-clone-inference" and generates the "genomic_df_subset.tsv" file.
Maybe remove this part from here (or vice versa) so it's not dubplaicate in the repo.

```{r echo=TRUE}
# Merge genomic_df_filter with pbta_subset to get sample_id information
# Create match_id2 to count for multiple samples per patient, if any
genomic_df_filter_merge <- genomic_df_filter %>% 
  left_join(pbta, by=c('Kids_First_Participant_ID'))

genomic_df_subset <- genomic_df_filter_merge %>%
  subset(select = c(Kids_First_Participant_ID, Kids_First_Biospecimen_ID, sample_id, composition, match_id, tumor_descriptor, descriptors, cancer_group.x, experimental_strategy.y)) %>%
  filter(!(experimental_strategy.y == "RNA-Seq")) %>%
  mutate(match_id2 = paste(Kids_First_Participant_ID, sample_id, composition, sep = "_")) %>%
  mutate(tumor_descriptor = case_when(grepl("Primary Tumor", tumor_descriptor) ~ "Diagnosis",
                                 grepl("Initial CNS Tumor", tumor_descriptor) ~ "Diagnosis",
                                 grepl('Progressive Disease Post-Mortem', tumor_descriptor) ~ 'Deceased', 
                                 TRUE ~ tumor_descriptor)) %>%
  write_tsv(file.path(results_dir, "genomic_df_subset.tsv"))

genomic_df_subset <- genomic_df_subset %>%
  select(Kids_First_Participant_ID, match_id2, Kids_First_Biospecimen_ID, descriptors, tumor_descriptor, cancer_group.x, experimental_strategy.y)  %>%
  arrange(Kids_First_Participant_ID) 

genomic_df_subset %>%
  group_by(Kids_First_Participant_ID, match_id2, descriptors) %>%
  tally() %>%
  regulartable() %>%
  fontsize(size = 12, part = "all")
```


```{r echo=TRUE}
maf_files <- snv_vaf_subset %>% 
  left_join(genomic_df_subset, by=c('Kids_First_Biospecimen_ID')) %>%
  filter(!is.na(Kids_First_Participant_ID))
  
# Create dir to save MAF files
temp_path_csv <-
  file.path(results_dir, "01_MAF_files_Per_Patient")
if (!dir.exists(temp_path_csv)) {
  dir.create(temp_path_csv)
}

# we will change the dir here otherwise files are saved outside of the desired folder
setwd(temp_path_csv)

# we will split masf data by the "Kids_First_Participant_ID.x" column
maf_files %>% 
  group_split(Kids_First_Participant_ID) %>% 
  walk(~write_csv(.x, paste0(.x$Kids_First_Participant_ID[1], "-vaf.csv")))

# let's change back to the root_dir for the rest of the module
setwd(root_dir)
```


```{r echo=TRUE}
sessionInfo()
```
  