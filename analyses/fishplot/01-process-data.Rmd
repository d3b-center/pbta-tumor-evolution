---
title: "Inference of subclonal architecture of tumors across multiple timepoints for the PBTA Cohort (pbta-tumor-evolution project)"
author: 'Antonia Chroni <chronia@chop.edu> and Jo Lynne Rokita <rokita@chop.edu> for D3B'
date: "2023"
output:
  html_notebook:
    toc: TRUE
    toc_float: TRUE
---


#### Tumor evolution project 

# Background
We are investigating the subclonal architecture of tumors across multiple timepoints in the PBTA cohort.
We will infer fishplots for each patient sample with multiple time points.

### Data used 
In this notebook, we are looking into the v12 histologies file (last updates on May 1st, 2023) and subset to the PBTA cohort.
We use the output files from the "analyses/sample-distribution-analysis/results".

### Method used 
For visualizing the tumor sublcones, we will use the fishplot R package as described in here: https://github.com/chrisamiller/fishplot


### Usage

This notebook is intended to be run via the command line from the top directory
of the repository as follows:

```
Rscript -e "rmarkdown::render('analyses/fishplot/01-fishplot.Rmd', clean = TRUE)"
```

# Set up

```{r load-library}
suppressPackageStartupMessages({
  library(tidyverse)
  library(data.table)
  library(R.utils)
  library(fishplot) # https://github.com/chrisamiller/fishplot
  library(clonevol)
  library(devtools)
  library(cDriver) # https://github.com/hanasusak/cDriver/
  library(purrr)
  library(flextable)
  library(plyr)
})
```

## Directories and File Inputs/Outputs

```{r set-dir-and-file-names}
# Detect the ".git" folder -- this will be in the project root directory.
# Use this as the root directory to ensure proper sourcing of functions no
# matter where this is called from
root_dir <- rprojroot::find_root(rprojroot::has_dir(".git"))
#root_dir <- setwd("/Users/chronia/CHOP/projects/pbta-tumor-evolution/pbta-tumor-evolution-analysis/analyses/fishplot")
setwd(root_dir)


# File path to input directory
input_dir <-
  file.path(root_dir, "input")

# Inputs
# input_snv <- source(file.path(root_dir, "data", "snv-consensus-plus-hotspots.maf.tsv.gz"))
input_snv <- file.path(input_dir, "snv-consensus-plus-hotspots.maf.tsv.gz")

# The following files were generated from the add-sample-distribution analysis
# to source files from there - TO FIX THIS LATER
input_cohort_pbta <- file.path(input_dir, "pbta.tsv")
input_genomic_df_filter <- file.path(input_dir, "genomic_df_filter.tsv")

# File path to results directory
results_dir <-
  file.path(root_dir, "results")
if (!dir.exists(results_dir)) {
  dir.create(results_dir)
}


# File path to plots directory
plots_dir <-
   file.path(root_dir, "plots")
 if (!dir.exists(plots_dir)) {
   dir.create(plots_dir)
 }
```


## Load data
```{r load-inputs-please-wait}
# Inputs
## Read MAF file
snv <- data.table::fread(input_snv, data.table = F)

## Read in input_cohort_pbta file
pbta <- readr::read_tsv(input_cohort_pbta, guess_max = 100000, show_col_types = FALSE)

## Read in genomic_df_filter file
genomic_df_filter <- readr::read_tsv(input_genomic_df_filter, guess_max = 100000, show_col_types = FALSE)
```


# Process Data: snv - MIGHT REMOVE THIS LATER

We need to process each input file and generate input files for the fishplot analysis.
We will first change the column name of the biospecimen column to match the one from the histolgies files.
Then, we will calculate VAFs and CCFs per tumor sample.

```{r process snv, echo=TRUE}
colnames(snv)[which(names(snv) == "Tumor_Sample_Barcode")] <- "Kids_First_Biospecimen_ID"
```  


```{r calculate VAF, echo=TRUE}
# using formula for VAF calculation
snv_vaf <- snv %>% mutate(VAF = t_alt_count / (t_ref_count + t_alt_count))
```


```{r calculate CCF, echo=TRUE}
# computational way by using cDriver R package
# if you encounter the following "Error: vector memory exhausted (limit reached?)"
# you may need to increase the memory in Renviron for this step
# https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
# gc() #to check the memory used and/or available
snv_vaf_ccf <- CCF(snv_vaf)

# using formula for CCF calculation
snv_vaf_ccf_formula <- snv_vaf_ccf %>% mutate(CCF_formula = (2*t_alt_count)/ (t_ref_count + t_alt_count))
```

Both methods for CCF calculation seem to return the same values. The one using the formula is faster and requires less memory.
For now, we will proceed with the results from cDriver R package.

```{r echo=TRUE}
# we will subset to variables we will interested to focus on for downstream analysis
snv_vaf_ccf_subset <- subset(snv_vaf_ccf_formula, 
                             select = c(Hugo_Symbol, Kids_First_Biospecimen_ID, Chromosome, Start_Position, End_Position, Reference_Allele, Tumor_Seq_Allele1, Tumor_Seq_Allele2, Variant_Type, Gene, VAF, CCF, CCF_formula, t_ref_count, t_alt_count)) 

```

# Process Data for CloneFinder
## Step 1

We need to generate the input files for CloneFinder.
CloneFinder requires at least 2 tumor samples per anatomical site.

We will first identify the number of tumor samples per patient and we will keep only those with more than 2 tumor samples.


```{r echo=TRUE}
# Merge genomic_df_filter with pbta_subset to get sample_id information
# Create matxh_id2 to count for multiple samples per patient, if any

genomic_df_filter_merge <- genomic_df_filter %>% 
  left_join(pbta, by=c('Kids_First_Participant_ID'))

genomic_df_subset <- genomic_df_filter_merge %>%
  subset(select = c(Kids_First_Participant_ID, Kids_First_Biospecimen_ID, sample_id, composition, match_id, tumor_descriptor, descriptors, cancer_group.x, experimental_strategy.y)) %>%
  filter(!(experimental_strategy.y == "RNA-Seq")) %>%
  mutate(match_id2 = paste(Kids_First_Participant_ID, sample_id, composition, sep = "_")) %>%
  mutate(tumor_descriptor = case_when(grepl("Primary Tumor", tumor_descriptor) ~ "Diagnosis",
                                 grepl("Initial CNS Tumor", tumor_descriptor) ~ "Diagnosis",
                                 grepl('Progressive Disease Post-Mortem', tumor_descriptor) ~ 'Deceased', 
                                 TRUE ~ tumor_descriptor)) %>%
  write_tsv(file.path(results_dir, "genomic_df_subset.tsv"))

genomic_df_subset <- genomic_df_subset %>%
  select(Kids_First_Participant_ID, match_id2, Kids_First_Biospecimen_ID, descriptors, tumor_descriptor, cancer_group.x, experimental_strategy.y)  %>%
  arrange(Kids_First_Participant_ID) 


genomic_df_subset %>%
  group_by(Kids_First_Participant_ID, match_id2, descriptors) %>%
  tally() %>%
  regulartable() %>%
  fontsize(size = 12, part = "all")


# the following shows number of samples per descent order
genomic_df_subset %>%
  group_by(Kids_First_Participant_ID, match_id2, descriptors) %>%
  tally() %>%
  arrange(desc(n)) %>% 
  regulartable() %>%
  fontsize(size = 12, part = "all")

# let's count tumor samples/patient
count <- genomic_df_subset %>% 
         group_by(Kids_First_Participant_ID, match_id2, Kids_First_Biospecimen_ID, descriptors, tumor_descriptor, cancer_group.x) %>%
         dplyr::count(match_id2) 

# keep only the rows with more than 2 occurrences in count$n
keep <- count[count$n > 1, ] %>%
  write_tsv(file.path(results_dir, "genomic_df_subset_keep.tsv"))

```

This results to 16 patients. 
We will use to filter and prepare input files for CloneFinder.

## Step 2
Input files for CloneFinder should contain the read counts for reference and alteration per each chromosomal position.
We will split these per patient sample based on the filtering from step 1.

```{r echo=TRUE}
# we will replace names in the column to follow input guidelines for CloneFinder
# For more details see here: https://github.com/SayakaMiura/CloneFinderPlus 
# we will use Tumor_Seq_Allele1 for now
# if we keep this analysis, we should repeat this step using Tumor_Seq_Allele2)
# or think of an alternative combining these two

read_counts_all_samples <- subset(snv_vaf_ccf_subset, 
                             select = c(Kids_First_Biospecimen_ID, Chromosome, Start_Position, Reference_Allele, Tumor_Seq_Allele1, t_ref_count, t_alt_count))

read_counts_all_samples <- setnames(read_counts_all_samples, old = c('Chromosome','Start_Position','Reference_Allele', "Tumor_Seq_Allele1", "t_ref_count", "t_alt_count"), 
         new = c('Chr','Position','Wild', "Mut", "XX:ref", "XX:alt")) 

read_counts_all_samples_keep <- read_counts_all_samples %>% 
  left_join(keep, by=c('Kids_First_Biospecimen_ID')) %>%
  filter(!is.na(Kids_First_Participant_ID.x))
  
  
 # write_tsv(file.path(results_dir, "read_counts_all_samples_keep.tsv")) # this is a large file, if not needed later, maybe not to save?
```

We will split and save read count data by the "Kids_First_Participant_ID.x" column

```{r echo=TRUE}
# Create dir to save read count data to be used as input for CloneFinder
temp_path_csv <-
  file.path(results_dir, "CloneFinder_input_read_count_csv")
if (!dir.exists(temp_path_csv)) {
  dir.create(temp_path_csv)
}

# we will split Read_count data by the "Kids_First_Participant_ID.x" column

read_counts_all_samples_keep %>% 
  group_split(Kids_First_Participant_ID.x) %>% 
  walk(~write_csv(.x, paste(temp_path_csv, sep = ".", .x$Kids_First_Participant_ID.x[1], "csv"))) #%>%

# this saves csv files outside of the "temp_path" folder 
# I manually moved them into the "temp_path" folder 
# TODO 
```

This results to 124 patients. 
This is not consistent with the "keep' df. --- WHAT HAPPENNED? TODO ---


### drop column
The following code might not be necessary but leave it here for now
```{r echo=TRUE}
# also I want to drop first column in each excel file
# but let's hope this will not be an issue for CloneFinder
# it is not an issue but the following code works, so elave it for now

# to drop "Kids_First_Biospecimen_ID" column
#data_files <- list.files(temp_path_csv)  # Identify file names
#data_files                                                    # Print file names

#dir.create("cleaned")
#files <- list.files(temp_path_csv, pattern="\\.csv$", full.names=TRUE)
#walk(files, function(x) {
#  DT <- fread(x, sep=",")
#  DT[, Kids_First_Biospecimen_ID := NULL]
#  fwrite(DT, file.path("cleaned", basename(x)), sep=",", col.names=TRUE, row.names=FALSE, quote=FALSE)
#})
```

## Step 3 

Next we need to create different columns assigning the read counts for ref and alt per tumor sample ("Kids_First_Biospecimen_ID")
It should follow this format per each tumor sample: XX:ref	XX:alt

TODO!!!

```{r echo=TRUE}

# Create dir to save read count data to be used as input for CloneFinder
#temp_path_txt <-
#  file.path(results_dir, "CloneFinder_input_read_count_txt")
#if (!dir.exists(temp_path_txt)) {
#  dir.create(temp_path_txt)
#}

# input files to be saved as txt files 
#walk(~write.table(.x, paste(temp_path_txt, sep = ".", .x$Kids_First_Biospecimen_ID[1], "txt")))
```

## Step 4 

```{r echo=TRUE}
# let's create a list of the samples and their directory path
# we will use this to run CloneFinder for all samples together

distinct_samples <- read_counts_all_samples_keep %>% distinct(Kids_First_Participant_ID.x)                   #removes the duplicate values

samples_list <-  subset(distinct_samples, 
                             select = c(Kids_First_Participant_ID.x)) %>%
           mutate(path = "/Users/chronia/CHOP/projects/pbta-tumor-evolution/pbta-tumor-evolution-analysis/analyses/fishplot/results/CloneFinder_input_read_count_csv",
                 additional = 1) %>%
  write_csv(file.path(results_dir, "distinct_samples_list.csv"))
```


```{r echo=TRUE}
# Create directory to save CloneFinder results
CloneFinder_results <-
  file.path(results_dir, "CloneFinder_results")
if (!dir.exists(CloneFinder_results)) {
  dir.create(CloneFinder_results)
}
```

```{r}
sessionInfo()
```
  